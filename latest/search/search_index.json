{
    "docs": [
        {
            "location": "/", 
            "text": "SOM.jl - Kohonen's self-organising maps for Julia\n\n\nThe package provides training and visualisation functions for Kohonen's self-organising maps for Julia. Training functions are implemented in pure Julia, without calling external libraries.\n\n\nVisualisation is implemented by using Python's MatPlotLib.\n\n\n\n\nSelf-organising maps\n\n\nSelf-organising maps (also referred to as SOMs or \nKohonen\n maps) are artificial neural networks introduced by Teuvo Kohonen in the 1980s. Despite of their age SOMs are still widely used as an easy and robust unsupervised learning algorithm for analysis and visualisation of high-dimensional data.\n\n\nThe SOM algorithm maps high-dimensional vectors into a lower-dimensional grid. Most often the target grid is two-dimensional, resulting into  intuitively interpretable maps.\n\n\nFor more details see Kohonen's papers, such as\n\n\n\n\nTeuvo Kohonen, \nBiological Cybernetics,\n \n43\n (1982) p. 59-69 Teuvo Kohonen, \nBiological Cybernetics,\n \n44\n (1982) p. 135-140\n\n\n\n\nTechnical details and background can be found in Kohonen's still relevant technical report:\n\n\n\n\nTeuvo Kohonen, Jussi Hynninen, Jari Kangas, and Jorma Laaksonen. \nSOM_PAK: The Self-Organizing Map Program Package.\n Technical Report A31, Helsinki University of Technology, Laboratory of Computer and Information Science, FIN-02150 Espoo, Finland, 1996. \nhttp://www.cis.hut.fi/research/papers/som_tr96.ps.Z\n\n\n\n\n\n\nInstallation\n\n\nBecause the package is not yet registered, it can be installed from the Julia REPL with:\n\n\nPkg.clone(\nhttps://github.com/andreasdominik/SOM.jl.git\n)\n\n\n\n\nThe package requires \nDataFrames, Distances, Distributions, ProgressMeter, StatsBase, PyPlot,\n and \nPyCall\n with the Python package \nmatplotlib\n to be installed. The requirements will be installed automatically by package manager. Sometimes \nmatplotlib\n causes problems. The easiest way of manual installation is from within Julia into the default Julia-Python environment via:\n\n\nENV[\nPYTHON\n]=\n\nPkg.add(\nConda\n)\nusing Conda\nConda.update()\n\nConda.add(\nmatplotlib\n)\nPkg.add(\nPyCall\n)\nPkg.build(\nPyCall\n)\nPkg.add(\nPyPlot\n);\n\n\n\n\n\n\nQuick Start\n\n\n\n\nQuick start: a first tutorial\n\n\n\n\n\n\nAPI\n\n\n\n\nTypes\n\n\nTraining\n\n\nKernel functions\n\n\nVisualisation\n\n\n\n\n\n\nIndex\n\n\n\n\nSOM.Som\n\n\nSOM.bubbleKernel\n\n\nSOM.classFrequencies\n\n\nSOM.gaussianKernel\n\n\nSOM.initSOM\n\n\nSOM.mapToSOM\n\n\nSOM.plotClasses\n\n\nSOM.plotDensity\n\n\nSOM.trainSOM", 
            "title": "Introduction"
        }, 
        {
            "location": "/#somjl-kohonens-self-organising-maps-for-julia", 
            "text": "The package provides training and visualisation functions for Kohonen's self-organising maps for Julia. Training functions are implemented in pure Julia, without calling external libraries.  Visualisation is implemented by using Python's MatPlotLib.", 
            "title": "SOM.jl - Kohonen's self-organising maps for Julia"
        }, 
        {
            "location": "/#self-organising-maps", 
            "text": "Self-organising maps (also referred to as SOMs or  Kohonen  maps) are artificial neural networks introduced by Teuvo Kohonen in the 1980s. Despite of their age SOMs are still widely used as an easy and robust unsupervised learning algorithm for analysis and visualisation of high-dimensional data.  The SOM algorithm maps high-dimensional vectors into a lower-dimensional grid. Most often the target grid is two-dimensional, resulting into  intuitively interpretable maps.  For more details see Kohonen's papers, such as   Teuvo Kohonen,  Biological Cybernetics,   43  (1982) p. 59-69 Teuvo Kohonen,  Biological Cybernetics,   44  (1982) p. 135-140   Technical details and background can be found in Kohonen's still relevant technical report:   Teuvo Kohonen, Jussi Hynninen, Jari Kangas, and Jorma Laaksonen.  SOM_PAK: The Self-Organizing Map Program Package.  Technical Report A31, Helsinki University of Technology, Laboratory of Computer and Information Science, FIN-02150 Espoo, Finland, 1996.  http://www.cis.hut.fi/research/papers/som_tr96.ps.Z", 
            "title": "Self-organising maps"
        }, 
        {
            "location": "/#installation", 
            "text": "Because the package is not yet registered, it can be installed from the Julia REPL with:  Pkg.clone( https://github.com/andreasdominik/SOM.jl.git )  The package requires  DataFrames, Distances, Distributions, ProgressMeter, StatsBase, PyPlot,  and  PyCall  with the Python package  matplotlib  to be installed. The requirements will be installed automatically by package manager. Sometimes  matplotlib  causes problems. The easiest way of manual installation is from within Julia into the default Julia-Python environment via:  ENV[ PYTHON ]= \nPkg.add( Conda )\nusing Conda\nConda.update()\n\nConda.add( matplotlib )\nPkg.add( PyCall )\nPkg.build( PyCall )\nPkg.add( PyPlot );", 
            "title": "Installation"
        }, 
        {
            "location": "/#quick-start", 
            "text": "Quick start: a first tutorial", 
            "title": "Quick Start"
        }, 
        {
            "location": "/#api", 
            "text": "Types  Training  Kernel functions  Visualisation", 
            "title": "API"
        }, 
        {
            "location": "/#index", 
            "text": "SOM.Som  SOM.bubbleKernel  SOM.classFrequencies  SOM.gaussianKernel  SOM.initSOM  SOM.mapToSOM  SOM.plotClasses  SOM.plotDensity  SOM.trainSOM", 
            "title": "Index"
        }, 
        {
            "location": "/tutorials/firstTutorial/", 
            "text": "Quick start: a first tutorial\n\n\nThe concept of SOM.jl follows Kohonen's SOM_PAK software by doing a 3-step approach:\n\n\n\n\ninitialise the SOM by defining topology, dimensions and random inital codebook vectors\n\n\ntrain the SOM in one or more rounds with different training parameters\n\n\nmap data into the Som and get visualisations.\n\n\n\n\nIn the following example the functions are called with a minimum set of parameters. For a description of all possible arguments see the API documentation.\n\n\n\n\nTraining data\n\n\nA first example uses the \nIris\n dataset, that comprises lengths and widths of petals and sepals of the blossoms of iris flowers of species \nIris viginica\n, \nIris setosa\n and \nIris versicolor\n. The dataset includes the 4 attributes and the correct species for 150 flowers:\n\n\njulia\n iris\n150\u00d75 DataFrames.DataFrame\n\u2502 Row \u2502 SepalLength \u2502 SepalWidth \u2502 PetalLength \u2502 PetalWidth \u2502 Species   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1   \u2502 5.1         \u2502 3.5        \u2502 1.4         \u2502 0.2        \u2502 setosa    \u2502\n\u2502 2   \u2502 4.9         \u2502 3.0        \u2502 1.4         \u2502 0.2        \u2502 setosa    \u2502\n\u2502 3   \u2502 4.7         \u2502 3.2        \u2502 1.3         \u2502 0.2        \u2502 setosa    \u2502\n\u2502 4   \u2502 4.6         \u2502 3.1        \u2502 1.5         \u2502 0.2        \u2502 setosa    \u2502\n\u2502 5   \u2502 5.0         \u2502 3.6        \u2502 1.4         \u2502 0.2        \u2502 setosa    \u2502\n\u2502 6   \u2502 5.4         \u2502 3.9        \u2502 1.7         \u2502 0.4        \u2502 setosa    \u2502\n\u2502 7   \u2502 4.6         \u2502 3.4        \u2502 1.4         \u2502 0.3        \u2502 setosa    \u2502\n...\n\n\n\n\nAs the training of self-organising maps is unsupervised, the class label (\nspecies\n) must be removed for training:\n\n\nusing SOM\nusing RDatasets\n\niris = dataset(\ndatasets\n, \niris\n)\ntrain = iris[:,1:4]\n\n\n\n\n\n\nSOM initialisation\n\n\nInitialisation sets the parameters of the SOM, such as dimensions, topology and normalisation as well as initial values of the codebook vectors.\n\n\nCalled with the minimal set of arguments topology defaults to \nhexagonal\n and \nnot toroidal\n and normalisation to \nz-score normalisation\n with \u03bc = 0.0 and \u03c3 = 1.0. In the example, the SOM will have\n\n\n\n\nan hexagonal grid with edges of size 10 \u00d7 8\n\n\ntraining data z-score normalised.\n\n\n\n\nThe training data must be provided to derive normalisation parameters and to initialise the codes to random values within the attribute space of the dataset:\n\n\nsom = initSOM(train, 10, 8)\n\n\n\n\n\n\nTraining\n\n\nSeveral training steps can be performed with different training parameters, such as training steps and training radius. Each step returns a \nnew\n object of type Som, so that the progress of training can be analysed later. Although by default the radius decreases in the course of training, it is often advantageous to finalise the training with an additional round with small radius:\n\n\nsom = trainSOM(som, train, 10000)\nsom = trainSOM(som, train, 10000, r = 3)\n\n\n\n\n\n\nVisualisation\n\n\nVisualisations include a density plot that displays the number of training samples mapped to each neuron and a classes plot that shows the class labels of training samples for every neuron as a pie chart:\n\n\nplotDensity(som)\n\nfreqs = classFrequencies(som, iris, :Species)\nplotClasses(som, freqs)\n\n\n\n\n\n\nSupported topologies\n\n\nDefault topology is a hexagonal grid with borders. By specifying \ntopol = :rectangular\n a rectangular grid is uses instead. For both grids an edge-less toroidal topology can be defined with \ntoroidal = true\n. In this case neurons on the edge of the SOM will neighbour the neurons on the opposite edge (simply spoken: the left is connected to the right and the top is connected with the bottom).\n\n\nIn addition spherical SOMs are possible.\n\n\nTraining and visualisations work for all supported topologies. Please refer to the API documentation for details.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/tutorials/firstTutorial/#quick-start-a-first-tutorial", 
            "text": "The concept of SOM.jl follows Kohonen's SOM_PAK software by doing a 3-step approach:   initialise the SOM by defining topology, dimensions and random inital codebook vectors  train the SOM in one or more rounds with different training parameters  map data into the Som and get visualisations.   In the following example the functions are called with a minimum set of parameters. For a description of all possible arguments see the API documentation.", 
            "title": "Quick start: a first tutorial"
        }, 
        {
            "location": "/tutorials/firstTutorial/#training-data", 
            "text": "A first example uses the  Iris  dataset, that comprises lengths and widths of petals and sepals of the blossoms of iris flowers of species  Iris viginica ,  Iris setosa  and  Iris versicolor . The dataset includes the 4 attributes and the correct species for 150 flowers:  julia  iris\n150\u00d75 DataFrames.DataFrame\n\u2502 Row \u2502 SepalLength \u2502 SepalWidth \u2502 PetalLength \u2502 PetalWidth \u2502 Species   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1   \u2502 5.1         \u2502 3.5        \u2502 1.4         \u2502 0.2        \u2502 setosa    \u2502\n\u2502 2   \u2502 4.9         \u2502 3.0        \u2502 1.4         \u2502 0.2        \u2502 setosa    \u2502\n\u2502 3   \u2502 4.7         \u2502 3.2        \u2502 1.3         \u2502 0.2        \u2502 setosa    \u2502\n\u2502 4   \u2502 4.6         \u2502 3.1        \u2502 1.5         \u2502 0.2        \u2502 setosa    \u2502\n\u2502 5   \u2502 5.0         \u2502 3.6        \u2502 1.4         \u2502 0.2        \u2502 setosa    \u2502\n\u2502 6   \u2502 5.4         \u2502 3.9        \u2502 1.7         \u2502 0.4        \u2502 setosa    \u2502\n\u2502 7   \u2502 4.6         \u2502 3.4        \u2502 1.4         \u2502 0.3        \u2502 setosa    \u2502\n...  As the training of self-organising maps is unsupervised, the class label ( species ) must be removed for training:  using SOM\nusing RDatasets\n\niris = dataset( datasets ,  iris )\ntrain = iris[:,1:4]", 
            "title": "Training data"
        }, 
        {
            "location": "/tutorials/firstTutorial/#som-initialisation", 
            "text": "Initialisation sets the parameters of the SOM, such as dimensions, topology and normalisation as well as initial values of the codebook vectors.  Called with the minimal set of arguments topology defaults to  hexagonal  and  not toroidal  and normalisation to  z-score normalisation  with \u03bc = 0.0 and \u03c3 = 1.0. In the example, the SOM will have   an hexagonal grid with edges of size 10 \u00d7 8  training data z-score normalised.   The training data must be provided to derive normalisation parameters and to initialise the codes to random values within the attribute space of the dataset:  som = initSOM(train, 10, 8)", 
            "title": "SOM initialisation"
        }, 
        {
            "location": "/tutorials/firstTutorial/#training", 
            "text": "Several training steps can be performed with different training parameters, such as training steps and training radius. Each step returns a  new  object of type Som, so that the progress of training can be analysed later. Although by default the radius decreases in the course of training, it is often advantageous to finalise the training with an additional round with small radius:  som = trainSOM(som, train, 10000)\nsom = trainSOM(som, train, 10000, r = 3)", 
            "title": "Training"
        }, 
        {
            "location": "/tutorials/firstTutorial/#visualisation", 
            "text": "Visualisations include a density plot that displays the number of training samples mapped to each neuron and a classes plot that shows the class labels of training samples for every neuron as a pie chart:  plotDensity(som)\n\nfreqs = classFrequencies(som, iris, :Species)\nplotClasses(som, freqs)", 
            "title": "Visualisation"
        }, 
        {
            "location": "/tutorials/firstTutorial/#supported-topologies", 
            "text": "Default topology is a hexagonal grid with borders. By specifying  topol = :rectangular  a rectangular grid is uses instead. For both grids an edge-less toroidal topology can be defined with  toroidal = true . In this case neurons on the edge of the SOM will neighbour the neurons on the opposite edge (simply spoken: the left is connected to the right and the top is connected with the bottom).  In addition spherical SOMs are possible.  Training and visualisations work for all supported topologies. Please refer to the API documentation for details.", 
            "title": "Supported topologies"
        }, 
        {
            "location": "/api/types/", 
            "text": "Types\n\n\n#\n\n\nSOM.Som\n \n \nType\n.\n\n\nstruct Som\n\n\n\n\nStored data of a trained SOM.\n\n\nFields:\n\n\n\n\ncodes\n: 2D-array of codebook vectors. One vector per row\n\n\ncolNames\n: names of the attribute with which the SOM is trained\n\n\nnormParams\n: DataFrame with normalisation parameters for each column               of training data. Column headers corresponds with               colNames.\n\n\nnorm\n: normalisation of training data; one of \n:none, :minmax, :zscore\n\n\nxdim\n: number of neurons in x-direction\n\n\nydim\n: number of neurons in y-direction\n\n\nnCodes\n: total number of neurons\n\n\ngrid\n: 2D-array of coordinates of neurons on the map         (2 columns (x,y)] for rectangular and hexagonal maps          3 columns (x,y,z) for spherical maps)\n\n\nindices\n: 2D-array of indices of the neurons\n\n\ntopol\n: topology of the SOM; one of \n:rectangular, :hexagonal, :spherical\n\n\ntoroidal\n: if \ntrue\n, the SOM is toroidal (no edges)\n\n\npopulation\n: 1D-array of numbers of training samples mapped to               each neuron.\n\n\n\n\nsource", 
            "title": "Types"
        }, 
        {
            "location": "/api/types/#types", 
            "text": "#  SOM.Som     Type .  struct Som  Stored data of a trained SOM.  Fields:   codes : 2D-array of codebook vectors. One vector per row  colNames : names of the attribute with which the SOM is trained  normParams : DataFrame with normalisation parameters for each column               of training data. Column headers corresponds with               colNames.  norm : normalisation of training data; one of  :none, :minmax, :zscore  xdim : number of neurons in x-direction  ydim : number of neurons in y-direction  nCodes : total number of neurons  grid : 2D-array of coordinates of neurons on the map         (2 columns (x,y)] for rectangular and hexagonal maps          3 columns (x,y,z) for spherical maps)  indices : 2D-array of indices of the neurons  topol : topology of the SOM; one of  :rectangular, :hexagonal, :spherical  toroidal : if  true , the SOM is toroidal (no edges)  population : 1D-array of numbers of training samples mapped to               each neuron.   source", 
            "title": "Types"
        }, 
        {
            "location": "/api/soms/", 
            "text": "Training\n\n\n#\n\n\nSOM.initSOM\n \n \nFunction\n.\n\n\ninitSOM(train, xdim, ydim = xdim;  norm = :zscore, topol = :hexagonal,\n        toroidal = false)\n\n\n\n\nInitialises a SOM.\n\n\nArguments:\n\n\n\n\ntrain\n: training data, must be convertable to Array{Float64,2}.\n\n\nxdim, ydim\n: geometry of the SOM          If DataFrame, the column names will be used as attribute names.          Codebook vectors will be sampled from the training data.          for spherical SOMs ydim can be omitted.\n\n\nnorm\n: optional normalisation; one of :\nminmax, :zscore or :none\n\n\ntopol\n: topology of the SOM; one of \n:rectangular, :hexagonal or :spherical\n.\n\n\ntoroidal\n: optional flag; if true, the SOM is toroidal.\n\n\n\n\nsource\n\n\n#\n\n\nSOM.trainSOM\n \n \nFunction\n.\n\n\ntrainSOM(som::Som, train::Any, len;\n         \u03b7 = 0.2 kernel = gaussianKernel,\n         r = 0.0, rDecay = true, \u03b7Decay = true)\n\n\n\n\nTrain an initialised or pre-trained SOM.\n\n\nArguments:\n\n\n\n\nsom\n: object of type Som with a trained som\n\n\ntrain\n: training DataType\n\n\nlen\n: number of training steps (\nnot\n epochs)\n\n\n\u03b7\n: learning rate. \u03b7 decays to 0.0 during the training\n\n\nkernel\n: optional distance kernel; one of (\nbubbleKernel, gaussianKernel\n)           default is \ngaussianKernel\n\n\nr\n: optional training radius.      If r is not specified, it defaults to \u221a(xdim^2 + ydim^2) / 2\n\n\nrDecay\n: optional flag; if true, r decays to 0.0 during the training.\n\n\n\u03b7Decay\n: optional flag; if true, learning rate \u03b7 decays to 0.0           during the training.\n\n\n\n\nsource\n\n\n#\n\n\nSOM.mapToSOM\n \n \nFunction\n.\n\n\nmapToSOM(som::Som, data)\n\n\n\n\nReturn a DataFrame with X-, Y-indices and index of winner neuron for every row in data.\n\n\nData must have the same number of dimensions as the training dataset.\n\n\nArguments\n\n\n\n\nsom\n: a trained SOM\n\n\ndata\n: Array or DataFrame with training data.\n\n\n\n\nsource\n\n\n#\n\n\nSOM.classFrequencies\n \n \nFunction\n.\n\n\nclassFrequencies(som::Som, data, classes)\n\n\n\n\nReturn a DataFrame with class frequencies for all neurons.\n\n\nArguments:\n\n\n\n\nsom\n: a trained SOM\n\n\ndata\n: data with row-wise samples and class information in each row\n\n\nclasses\n: Name of column with class information.\n\n\n\n\nData must have the same number of dimensions as the training dataset. The column with class labels is given as \nclasses\n (name or index). Returned DataFrame has the columns:\n\n\n\n\nX-, Y-indices and index: of winner neuron for every row in data\n\n\npopulation: number of samples mapped to the neuron\n\n\nfrequencies: one column for each class label.\n\n\n\n\nsource\n\n\n\n\nKernel functions\n\n\n#\n\n\nSOM.bubbleKernel\n \n \nFunction\n.\n\n\nbubbleKernel(x, r)\n\n\n\n\nReturn 1.0 if dist \n= r, else 0.0.\n\n\nsource\n\n\n#\n\n\nSOM.gaussianKernel\n \n \nFunction\n.\n\n\ngaussianKernel(x, r)\n\n\n\n\nReturn the Gaussian for x  and \u03c3 = r/3.\n\n\nsource", 
            "title": "Training"
        }, 
        {
            "location": "/api/soms/#training", 
            "text": "#  SOM.initSOM     Function .  initSOM(train, xdim, ydim = xdim;  norm = :zscore, topol = :hexagonal,\n        toroidal = false)  Initialises a SOM.  Arguments:   train : training data, must be convertable to Array{Float64,2}.  xdim, ydim : geometry of the SOM          If DataFrame, the column names will be used as attribute names.          Codebook vectors will be sampled from the training data.          for spherical SOMs ydim can be omitted.  norm : optional normalisation; one of : minmax, :zscore or :none  topol : topology of the SOM; one of  :rectangular, :hexagonal or :spherical .  toroidal : optional flag; if true, the SOM is toroidal.   source  #  SOM.trainSOM     Function .  trainSOM(som::Som, train::Any, len;\n         \u03b7 = 0.2 kernel = gaussianKernel,\n         r = 0.0, rDecay = true, \u03b7Decay = true)  Train an initialised or pre-trained SOM.  Arguments:   som : object of type Som with a trained som  train : training DataType  len : number of training steps ( not  epochs)  \u03b7 : learning rate. \u03b7 decays to 0.0 during the training  kernel : optional distance kernel; one of ( bubbleKernel, gaussianKernel )           default is  gaussianKernel  r : optional training radius.      If r is not specified, it defaults to \u221a(xdim^2 + ydim^2) / 2  rDecay : optional flag; if true, r decays to 0.0 during the training.  \u03b7Decay : optional flag; if true, learning rate \u03b7 decays to 0.0           during the training.   source  #  SOM.mapToSOM     Function .  mapToSOM(som::Som, data)  Return a DataFrame with X-, Y-indices and index of winner neuron for every row in data.  Data must have the same number of dimensions as the training dataset.  Arguments   som : a trained SOM  data : Array or DataFrame with training data.   source  #  SOM.classFrequencies     Function .  classFrequencies(som::Som, data, classes)  Return a DataFrame with class frequencies for all neurons.  Arguments:   som : a trained SOM  data : data with row-wise samples and class information in each row  classes : Name of column with class information.   Data must have the same number of dimensions as the training dataset. The column with class labels is given as  classes  (name or index). Returned DataFrame has the columns:   X-, Y-indices and index: of winner neuron for every row in data  population: number of samples mapped to the neuron  frequencies: one column for each class label.   source", 
            "title": "Training"
        }, 
        {
            "location": "/api/soms/#kernel-functions", 
            "text": "#  SOM.bubbleKernel     Function .  bubbleKernel(x, r)  Return 1.0 if dist  = r, else 0.0.  source  #  SOM.gaussianKernel     Function .  gaussianKernel(x, r)  Return the Gaussian for x  and \u03c3 = r/3.  source", 
            "title": "Kernel functions"
        }, 
        {
            "location": "/api/visualisations/", 
            "text": "Visualisation\n\n\n#\n\n\nSOM.plotDensity\n \n \nFunction\n.\n\n\nplotDensity(som::Som; predict = nothing,\n            title = \nsomPlot\n, paper = :a4r,\n            colormap = \nautumn_r\n,\n            device = :display, fileName = \nsomplot\n)\n\n\n\n\nPlot the population of neurons as colours.\n\n\nArguments:\n\n\n\n\nsom\n: the som of type \nSom\n; som is the only mandatory argument\n\n\npredict\n: DataFrame of mappings as outputed by winners()\n\n\ntitle\n: main title of plot\n\n\npaper\n: plot size; currentlx supported: \n:a4, :a4r, :letter, :letterr\n\n\ncolormap\n: MatPlotLib colourmap (Python-style as string \n\"gray\"\n or             Julia-style as Symbol \n:gray\n)\n\n\ndevice\n: one of \n:display, :png, :svg, :pdf\n or any file-type supported           by MatPlotLib; default is \n:display\n\n\nfileName\n: name of image file. File extention overrides the setting of             \ndevice\n.\n\n\n\n\nsource\n\n\n#\n\n\nSOM.plotClasses\n \n \nFunction\n.\n\n\nplotClasses(som::Som, frequencies;\n            title = \nsomPlot\n, paper = :a4r,\n            colormap = \nrainbow\n,\n            device = :display, fileName = \nsomplot\n)\n\n\n\n\nPlot the population of neurons as colours.\n\n\nArguments:\n\n\n\n\nsom\n: the som of type \nSom\n; som is the only mandatory argument\n\n\nfrequencies\n: DataFrame of frequencies as outputed by classFrequencies()\n\n\ntitle\n: main title of plot\n\n\npaper\n: plot size; currentlx supported: \n:a4, :a4r, :letter, :letterr\n\n\ncolors\n: MatPlotLib colourmap (Python-style as string \n\"gray\"\n or             Julia-style as Symbol \n:gray\n) \nor\n dictionary with             classes as keys and colours as vals; default: \nbrg\n\n\ndevice\n: one of \n:display, :png, :svg, :pdf\n or any file-type supported           by MatPlotLib; default is \n:display\n\n\nfileName\n: name of image file. File extention overrides the setting of             \ndevice\n.\n\n\n\n\nsource", 
            "title": "Visualisation"
        }, 
        {
            "location": "/api/visualisations/#visualisation", 
            "text": "#  SOM.plotDensity     Function .  plotDensity(som::Som; predict = nothing,\n            title =  somPlot , paper = :a4r,\n            colormap =  autumn_r ,\n            device = :display, fileName =  somplot )  Plot the population of neurons as colours.  Arguments:   som : the som of type  Som ; som is the only mandatory argument  predict : DataFrame of mappings as outputed by winners()  title : main title of plot  paper : plot size; currentlx supported:  :a4, :a4r, :letter, :letterr  colormap : MatPlotLib colourmap (Python-style as string  \"gray\"  or             Julia-style as Symbol  :gray )  device : one of  :display, :png, :svg, :pdf  or any file-type supported           by MatPlotLib; default is  :display  fileName : name of image file. File extention overrides the setting of              device .   source  #  SOM.plotClasses     Function .  plotClasses(som::Som, frequencies;\n            title =  somPlot , paper = :a4r,\n            colormap =  rainbow ,\n            device = :display, fileName =  somplot )  Plot the population of neurons as colours.  Arguments:   som : the som of type  Som ; som is the only mandatory argument  frequencies : DataFrame of frequencies as outputed by classFrequencies()  title : main title of plot  paper : plot size; currentlx supported:  :a4, :a4r, :letter, :letterr  colors : MatPlotLib colourmap (Python-style as string  \"gray\"  or             Julia-style as Symbol  :gray )  or  dictionary with             classes as keys and colours as vals; default:  brg  device : one of  :display, :png, :svg, :pdf  or any file-type supported           by MatPlotLib; default is  :display  fileName : name of image file. File extention overrides the setting of              device .   source", 
            "title": "Visualisation"
        }, 
        {
            "location": "/LICENSE/", 
            "text": "The SOM.jl package is licensed under the MIT \"Expat\" License:\n\n\n\n\nCopyright (c) 2018: Andreas Dominik.\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", 
            "title": "License"
        }
    ]
}